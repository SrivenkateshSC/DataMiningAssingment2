{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WfrCFmLHxYu"
   },
   "source": [
    "# CS6405 - Data Mining - Second Assignment\n",
    "\n",
    "\n",
    "### Declaration\n",
    "\n",
    "By submitting this assignment. I agree to the following:\n",
    "\n",
    "<font color=\"red\">“I have read and understand the UCC academic policy on plagiarism, and agree to the requirements set out thereby in relation to plagiarism and referencing. I confirm that I have referenced and acknowledged properly all sources used in the preparation of this assignment.\n",
    "I declare that this assignment is entirely my own work based on my personal study. I further declare that I have not engaged the services of another to either assist me in, or complete this assignment”</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the necessary libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oav9G1WSJ1nH"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "DE0kM0QsJ1En",
    "outputId": "8e1bade4-15c6-48a2-ebf6-98bf08c6cff0"
   },
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "# df = pd.read_csv()\n",
    "# df = pd.read_csv(\"https://github.com/andvise/DataAnalyticsDatasets/blob/6d5738101d173b97c565f143f945dedb9c42a400/dm_assignment2/sat_dataset_train.csv?raw=true\")\n",
    "original_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the columns with more than 40% NaN values\n",
    "\n",
    "#Columns with NaN values along with their sum\n",
    "df.columns[df.isna().any()].tolist()\n",
    "df[df.columns[df.isna().any()].tolist()].isna().sum()\n",
    "\n",
    "#Columns list with NaN values in all rows and more than 70% of the rows\n",
    "Complete_NaNColumns = ['v_nd_p_weights_entropy','v_nd_n_weights_entropy','c_nd_p_weights_entropy','c_nd_n_weights_entropy', 'band_node_entropy',\n",
    "                      'band_weights_entropy', 'exo_node_entropy', 'exo_weights_entropy', 'and_node_entropy', 'and_weights_entropy']\n",
    "\n",
    "df.drop(columns=Complete_NaNColumns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Replacing the NaN values with the mean value in Columns cg_al_node_entropy,\n",
    "cg_al_weights_entropy,rg_node_entropy,rg_weights_entropy'''\n",
    "\n",
    "df['cg_al_node_entropy'] = df['cg_al_node_entropy'].fillna(df['cg_al_node_entropy'].mean())\n",
    "df['cg_al_weights_entropy'] = df['cg_al_weights_entropy'].fillna(df['cg_al_weights_entropy'].mean())\n",
    "df['rg_node_entropy'] = df['rg_node_entropy'].fillna(df['rg_node_entropy'].mean())\n",
    "df['rg_weights_entropy'] = df['rg_weights_entropy'].fillna(df['rg_weights_entropy'].mean())\n",
    "df['big_node_entropy'] = df['big_node_entropy'].fillna(df['big_node_entropy'].mean())\n",
    "df['big_weights_entropy'] = df['big_weights_entropy'].fillna(df['big_weights_entropy'].mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No columns with NaN values\n",
    "df.columns[df.isna().any()].tolist()\n",
    "\n",
    "#Dropping all rows with inf/-inf values \n",
    "df = df[np.isfinite(df).all(1)]\n",
    "\n",
    "#rounding the values to avoid Large values \n",
    "df = round(df,100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOqpQTS-K8EN",
    "outputId": "18df9793-c50b-460e-f699-2424e4c370c0"
   },
   "outputs": [],
   "source": [
    "#Splitting the predictors and response variable\n",
    "\n",
    "Y = df['target']\n",
    "X = df\n",
    "X.drop(\"target\",axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTvkBPQvITf-"
   },
   "source": [
    "# Tasks\n",
    "\n",
    "## Basic models and evaluation (5 Marks)\n",
    "\n",
    "Using Scikit-learn, train and evaluate K-NN and decision tree classifiers using 70% of the dataset from training and 30% for testing. For this part of the project, we are not interested in optimising the parameters; we just want to get an idea of the dataset. Compare the results of both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "id": "Zl0VXO0YH1nG"
   },
   "outputs": [],
   "source": [
    "#Splitting the data into train and test sets\n",
    "#Holdout method\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=10, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN Method\n",
    "\n",
    "basic_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "basic_knn.fit(x_train, y_train)\n",
    "\n",
    "basic_knn_pred = basic_knn.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "\n",
    "basic_decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "basic_decision_tree.fit(x_train, y_train)\n",
    "\n",
    "basic_decision_tree_pred = basic_decision_tree.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Model Evaluations:\n",
      "=========================\n",
      "\n",
      "KNN Model:\n",
      "\n",
      "Confusion Matrix:\n",
      "[[253  25]\n",
      " [ 35 158]]\n",
      "Accuracy of the model: 87.26115\n",
      "\n",
      "Decision tree Model\n",
      "\n",
      "Confusion Matrix:\n",
      "[[277   1]\n",
      " [ 11 182]]\n",
      "Accuracy of the model: 97.45223\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "print('Basic Model Evaluations:')\n",
    "print('=========================')\n",
    "print('\\nKNN Model:\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(metrics.confusion_matrix(y_test,basic_knn_pred))\n",
    "print('Accuracy of the model:', round(metrics.accuracy_score(y_test,basic_knn_pred) * 100, 5))\n",
    "\n",
    "#Decision Tree\n",
    "print('\\nDecision tree Model\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(metrics.confusion_matrix(y_test,basic_decision_tree_pred))\n",
    "print('Accuracy of the model:', round(metrics.accuracy_score(y_test,basic_decision_tree_pred) * 100, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zADpr0f8IcGL"
   },
   "source": [
    "## Robust evaluation (10 Marks)\n",
    "\n",
    "In this section, we are interested in more rigorous techniques by implementing more sophisticated methods, for instance:\n",
    "* Hold-out and cross-validation.\n",
    "* Hyper-parameter tuning.\n",
    "* Feature reduction.\n",
    "* Feature normalisation.\n",
    "\n",
    "Your report should provide concrete information of your reasoning; everything should be well-explained.\n",
    "\n",
    "Do not get stressed if the things you try do not improve the accuracy. The key to geting good marks is to show that you evaluated different methods and that you correctly selected the configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Holdout and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Holdout Cross-validation method\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=10, stratify=Y)\n",
    "x_train_cv, x_test_cv, y_train_cv, y_test_cv = train_test_split(x_train, y_train, random_state=10, \n",
    "                                                                test_size=0.3, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "id": "tvBZH6ilInsA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the KNN model on Holdout-CV data:\n",
      "84.54545\n",
      "Accuracy of the KNN model on test data:\n",
      "84.92569\n",
      "\n",
      "\n",
      "Accuracy of the Decision tree model on Holdout-CV data:\n",
      "96.9697\n",
      "Accuracy of the Decision tree model on test data:\n",
      "96.60297\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "\n",
    "HoldoutCV_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "HoldoutCV_knn.fit(x_train_cv, y_train_cv)\n",
    "\n",
    "HoldoutCV_knn_pred = HoldoutCV_knn.predict(x_test_cv)\n",
    "\n",
    "print('Accuracy of the KNN model on Holdout-CV data:')\n",
    "print(round(metrics.accuracy_score(y_test_cv, HoldoutCV_knn_pred) * 100, 5))\n",
    "\n",
    "\n",
    "\n",
    "HoldoutCV_test_knn_pred = HoldoutCV_knn.predict(x_test)\n",
    "\n",
    "print('Accuracy of the KNN model on test data:')\n",
    "print(round(metrics.accuracy_score(y_test, HoldoutCV_test_knn_pred) * 100, 5))\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "#Decision Tree\n",
    "\n",
    "HoldoutCV_Dtree = DecisionTreeClassifier()\n",
    "\n",
    "HoldoutCV_Dtree.fit(x_train_cv, y_train_cv)\n",
    "\n",
    "HoldoutCV_Dtree_pred = HoldoutCV_Dtree.predict(x_test_cv)\n",
    "\n",
    "print('Accuracy of the Decision tree model on Holdout-CV data:')\n",
    "print(round(metrics.accuracy_score(y_test_cv, HoldoutCV_Dtree_pred) * 100, 5))\n",
    "\n",
    "\n",
    "HoldoutCV_test_Dtree_pred = HoldoutCV_Dtree.predict(x_test)\n",
    "\n",
    "print('Accuracy of the Decision tree model on test data:')\n",
    "print(round(metrics.accuracy_score(y_test, HoldoutCV_test_Dtree_pred) * 100, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'distance'}\n",
      "\n",
      "Accuracy of the KNN model on tuning Hyperparameters:\n",
      "88.1104\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "knn_grid_params = { 'n_neighbors' : [3,5,7,9,11],\n",
    "               'weights' : ['uniform','distance'],\n",
    "               'metric' : ['minkowski','euclidean','manhattan']}\n",
    "\n",
    "knn_tune = KNeighborsClassifier()\n",
    "\n",
    "#gridsearch for hyperparameters with 10-fold CV\n",
    "knn_tune_bestModel = GridSearchCV(knn_tune, knn_grid_params, cv=10)\n",
    "\n",
    "knn_tune_bestModel.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best Hyper Parameters:\\n\", knn_tune_bestModel.best_params_)\n",
    "\n",
    "\n",
    "knn_tune_bestModel_Pred = knn_tune_bestModel.predict(x_test)\n",
    "\n",
    "print('\\nAccuracy of the KNN model on tuning Hyperparameters:')\n",
    "print(round(metrics.accuracy_score(y_test, knn_tune_bestModel_Pred) * 100, 5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyper Parameters:\n",
      " {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'auto', 'min_impurity_decrease': 0.01, 'min_samples_leaf': 10}\n",
      "\n",
      "Accuracy of the KNN model on tuning Hyperparameters:\n",
      "97.0276\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "\n",
    "dtree_grid_params = {\n",
    "    'max_depth': [2, 3, 5, 10],\n",
    "    'min_samples_leaf': [5, 10, 20],\n",
    "    'criterion': [\"gini\", \"entropy\"], \n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_impurity_decrease' : [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "\n",
    "dtree_tune = DecisionTreeClassifier()\n",
    "\n",
    "#gridsearch for hyperparameters with 10-fold CV\n",
    "dtree_tune_bestModel = GridSearchCV(dtree_tune, dtree_grid_params, cv=10)\n",
    "\n",
    "dtree_tune_bestModel.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best Hyper Parameters:\\n\", dtree_tune_bestModel.best_params_)\n",
    "\n",
    "\n",
    "dtree_tune_bestModel_Pred = dtree_tune_bestModel.predict(x_test)\n",
    "\n",
    "print('\\nAccuracy of the KNN model on tuning Hyperparameters:')\n",
    "print(round(metrics.accuracy_score(y_test, dtree_tune_bestModel_Pred) * 100, 5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Feature Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=10, stratify=Y)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x_train_minmax = x_train\n",
    "x_test_minmax = x_test\n",
    "\n",
    "#Using min-max normalisation\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "x_train_minmax = min_max_scaler.fit_transform(x_train_minmax)\n",
    "x_test_minmax = min_max_scaler.fit_transform(x_test_minmax)\n",
    "\n",
    "\n",
    "#Using Z-score Normalisation\n",
    "stnd_scalar = StandardScaler()\n",
    "\n",
    "x_train_stnd = x_train\n",
    "x_test_stnd = x_test\n",
    "\n",
    "x_train_stnd = stnd_scalar.fit_transform(x_train_stnd)\n",
    "x_test_stnd = stnd_scalar.fit_transform(x_test_stnd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the KNN model using min-max normalisation on features:\n",
      "96.60297\n",
      "Accuracy of the KNN model using Z-score normalisation on features:\n",
      "96.39066\n",
      "\n",
      "\n",
      "Accuracy of the Decision tree model using min-max normalisation on features:\n",
      "76.43312\n",
      "Accuracy of the Decision tree model using Z-score normalisation on features:\n",
      "87.47346\n"
     ]
    }
   ],
   "source": [
    "#KNN Using Feature Normalisation\n",
    "#min-max Normalisation\n",
    "\n",
    "\n",
    "knn_min_max = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "knn_min_max.fit(x_train_minmax, y_train)\n",
    "\n",
    "knn_min_max_pred = knn_min_max.predict(x_test_minmax)\n",
    "\n",
    "print('Accuracy of the KNN model using min-max normalisation on features:')\n",
    "print(round(metrics.accuracy_score(y_test, knn_min_max_pred) * 100, 5))\n",
    "\n",
    "\n",
    "#Standard Normalisation\n",
    "knn_stnd = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "knn_stnd.fit(x_train_stnd, y_train)\n",
    "\n",
    "knn_stnd_pred = knn_stnd.predict(x_test_stnd)\n",
    "\n",
    "print('Accuracy of the KNN model using Z-score normalisation on features:')\n",
    "print(round(metrics.accuracy_score(y_test, knn_stnd_pred) * 100, 5))\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "#Decision Tree Using Feature Normalisation\n",
    "#min-max Normalisation\n",
    "\n",
    "\n",
    "dtree_min_max = DecisionTreeClassifier()\n",
    "\n",
    "dtree_min_max.fit(x_train_minmax, y_train)\n",
    "\n",
    "dtree_min_max_pred = dtree_min_max.predict(x_test_minmax)\n",
    "\n",
    "print('Accuracy of the Decision tree model using min-max normalisation on features:')\n",
    "print(round(metrics.accuracy_score(y_test, dtree_min_max_pred) * 100, 5))\n",
    "\n",
    "\n",
    "#Standard Normalisation\n",
    "dtree_stnd = DecisionTreeClassifier()\n",
    "\n",
    "dtree_stnd.fit(x_train_stnd, y_train)\n",
    "\n",
    "dtree_stnd_pred = dtree_stnd.predict(x_test_stnd)\n",
    "\n",
    "print('Accuracy of the Decision tree model using Z-score normalisation on features:')\n",
    "print(round(metrics.accuracy_score(y_test, dtree_stnd_pred) * 100, 5))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a. Feature Reduction - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature reduction on Z-score normalised data\n",
    "\n",
    "pca = PCA(n_components = 30)\n",
    "\n",
    "np.random.seed(42)\n",
    "x_train_pca = pca.fit_transform(x_train_stnd)\n",
    "x_test_pca = pca.fit_transform(x_test_stnd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the KNN model using feature reduction - PCA:\n",
      "78.55626\n",
      "Accuracy of the Decision tree model using feature reduction - PCA:\n",
      "55.62633\n"
     ]
    }
   ],
   "source": [
    "#KNN (K=3) Using Feature Reduction - PCA\n",
    "\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "knn_pca.fit(x_train_pca, y_train)\n",
    "\n",
    "knn_pca_pred = knn_pca.predict(x_test_pca)\n",
    "\n",
    "print('Accuracy of the KNN model using feature reduction - PCA:')\n",
    "print(round(metrics.accuracy_score(y_test, knn_pca_pred) * 100, 5))\n",
    "\n",
    "#Decision tree\n",
    "#Decision tree Using Feature Reduction - PCA\n",
    "\n",
    "dtree_pca = DecisionTreeClassifier()\n",
    "\n",
    "dtree_pca.fit(x_train_pca, y_train)\n",
    "\n",
    "dtree_pca_pred = dtree_pca.predict(x_test_pca)\n",
    "\n",
    "print('Accuracy of the Decision tree model using feature reduction - PCA:')\n",
    "print(round(metrics.accuracy_score(y_test, dtree_pca_pred) * 100, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4c. Above combined techniques along with the Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Model:\n",
      "Best Hyper Parameters for KNN:\n",
      " {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "\n",
      "Accuracy of the KNN model on tuning Hyperparameters after pre-processing:\n",
      "96.60297\n",
      "\n",
      "\n",
      "Decision Tree Model:\n",
      "Best Hyper Parameters for Decision tree:\n",
      " {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'sqrt', 'min_impurity_decrease': 0.01, 'min_samples_leaf': 5}\n",
      "\n",
      "Accuracy of the Decision Tree model on tuning Hyperparameters after pre-processing:\n",
      "91.50743\n"
     ]
    }
   ],
   "source": [
    "stnd_scalar = StandardScaler()\n",
    "\n",
    "x_train_stnd = x_train\n",
    "x_test_stnd = x_test\n",
    "\n",
    "x_train_stnd = stnd_scalar.fit_transform(x_train_stnd)\n",
    "x_test_stnd = stnd_scalar.fit_transform(x_test_stnd)\n",
    "\n",
    "#KNN\n",
    "knn_grid_params = { 'n_neighbors' : [3,5,7,9,11],\n",
    "               'weights' : ['uniform','distance'],\n",
    "               'metric' : ['minkowski','euclidean','manhattan']}\n",
    "\n",
    "knn_final_tune = KNeighborsClassifier()\n",
    "\n",
    "#gridsearch for hyperparameters with 10-fold CV\n",
    "knn_final_tuning = GridSearchCV(knn_final_tune, knn_grid_params, cv=10)\n",
    "\n",
    "knn_final_tuning.fit(x_train_stnd, y_train)\n",
    "\n",
    "print('KNN Model:')\n",
    "print(\"Best Hyper Parameters for KNN:\\n\", knn_final_tuning.best_params_)\n",
    "\n",
    "\n",
    "knn_final_tuning_pred = knn_final_tuning.predict(x_test_stnd)\n",
    "\n",
    "print('\\nAccuracy of the KNN model on tuning Hyperparameters after pre-processing:')\n",
    "print(round(metrics.accuracy_score(y_test, knn_final_tuning_pred) * 100, 5))\n",
    "\n",
    "\n",
    "#Decision Tree\n",
    "\n",
    "dtree_grid_params = {\n",
    "    'max_depth': [2, 3, 5, 10],\n",
    "    'min_samples_leaf': [5, 10, 20],\n",
    "    'criterion': [\"gini\", \"entropy\"], \n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_impurity_decrease' : [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "\n",
    "dtree_final_tuneModel = DecisionTreeClassifier()\n",
    "\n",
    "#gridsearch for hyperparameters with 10-fold CV\n",
    "dtree_final_tuning = GridSearchCV(dtree_final_tuneModel, dtree_grid_params, cv=10)\n",
    "\n",
    "dtree_final_tuning.fit(x_train_stnd, y_train)\n",
    "\n",
    "print('\\n\\nDecision Tree Model:')\n",
    "print(\"Best Hyper Parameters for Decision tree:\\n\", dtree_final_tuning.best_params_)\n",
    "\n",
    "\n",
    "dtree_final_tuning_pred = dtree_final_tuning.predict(x_test_stnd)\n",
    "\n",
    "print('\\nAccuracy of the Decision Tree model on tuning Hyperparameters after pre-processing:')\n",
    "print(round(metrics.accuracy_score(y_test, dtree_final_tuning_pred) * 100, 5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPORT DATAFRAME\n",
    "\n",
    "\n",
    "report_data = {'Techniques': ['Basic evaluation', 'Hold-out and cross-validation', 'Hold-out and cross-validation Test data',\n",
    "                              'Feature reduction - PCA', 'Feature Normalisation - minmax', 'Feature Normalisation - Standard',\n",
    "                             'Hyperparameter tuning', 'Final Model (Hyperparameter on pre-proc. data)'], \n",
    "               \n",
    "               'KNN Model': [87.26115,84.54545,84.92569,78.55626,96.60297,96.39066,88.1104,96.60297],\n",
    "               'Decision Tree Model': [97.45223,96.9697,96.60297,57.11253,76.43312,87.47346,97.0276,91.50743]}  \n",
    "\n",
    "report = pd.DataFrame(report_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PLEASE RUN THE BELOW CELL FOR THE RESULTS DISPLAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Techniques</th>\n",
       "      <th>KNN Model</th>\n",
       "      <th>Decision Tree Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic evaluation</td>\n",
       "      <td>87.26115</td>\n",
       "      <td>97.45223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hold-out and cross-validation</td>\n",
       "      <td>84.54545</td>\n",
       "      <td>96.96970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hold-out and cross-validation Test data</td>\n",
       "      <td>84.92569</td>\n",
       "      <td>96.60297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feature reduction - PCA</td>\n",
       "      <td>78.55626</td>\n",
       "      <td>57.11253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feature Normalisation - minmax</td>\n",
       "      <td>96.60297</td>\n",
       "      <td>76.43312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Feature Normalisation - Standard</td>\n",
       "      <td>96.39066</td>\n",
       "      <td>87.47346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hyperparameter tuning</td>\n",
       "      <td>88.11040</td>\n",
       "      <td>97.02760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Final Model (Hyperparameter on pre-proc. data)</td>\n",
       "      <td>96.60297</td>\n",
       "      <td>91.50743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Techniques  KNN Model  \\\n",
       "0                                Basic evaluation   87.26115   \n",
       "1                   Hold-out and cross-validation   84.54545   \n",
       "2         Hold-out and cross-validation Test data   84.92569   \n",
       "3                         Feature reduction - PCA   78.55626   \n",
       "4                  Feature Normalisation - minmax   96.60297   \n",
       "5                Feature Normalisation - Standard   96.39066   \n",
       "6                           Hyperparameter tuning   88.11040   \n",
       "7  Final Model (Hyperparameter on pre-proc. data)   96.60297   \n",
       "\n",
       "   Decision Tree Model  \n",
       "0             97.45223  \n",
       "1             96.96970  \n",
       "2             96.60297  \n",
       "3             57.11253  \n",
       "4             76.43312  \n",
       "5             87.47346  \n",
       "6             97.02760  \n",
       "7             91.50743  "
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report on Task 2:\n",
    "\n",
    "In basic evaluation, Holdout Cross validation methods, Decision tree classifier has performed better than the KNN model. We have splitted the data as 70% training and 30% testing data respectively. \n",
    "\n",
    "PCA was used as feature reduction technique and for Feature normalisation technique, both the min-max scaling and z-score normalization have been used here.\n",
    "Accuracy was reduced for decision tree model in the above both techniques, whereas KNN Model got enhanced and perfomed better on after the above techniques were implemented.\n",
    "\n",
    "In hyperparameter tuning, we found K=7 as optimal parameter for the KNN model and below parameters in case of Decision tree model. Decision tree performed better here and the acccuracy was improved and reached 97.02760%.\n",
    "\n",
    "Decision tree best parameters- {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'auto', 'min_impurity_decrease': 0.01, 'min_samples_leaf': 10}\n",
    "\n",
    "In the final model, we have implemented the hyperparameter tuning on the above pre-processed data. For pre-processing the data, PCA was applied on the training data, after which the data was processed using standard normalization technique.\n",
    "Here, KNN has achieved an highest accuracy of 96.60297%, more than the Decision tree model.\n",
    "\n",
    "We can conclude from the above results dataframe that the KNN model was a better fit the dataset and achieved a better accuracy after performing the pre-processing of the data and hyperparameter tuning on the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYoMg0EZIrNd"
   },
   "source": [
    "## New classifier (10 Marks)\n",
    "\n",
    "Replicate the previous task for a classifier that we did not cover in class. So different than K-NN and decision trees. Briefly describe your choice.\n",
    "Try to create the best model for the given dataset.\n",
    "Save your best model into your github. And create a single code cell that loads it and evaluate it on the following test dataset:\n",
    "https://github.com/andvise/DataAnalyticsDatasets/blob/main/dm_assignment2/sat_dataset_test.csv\n",
    "\n",
    "This link currently contains a sample of the training set. The real test set will be released after the submission. I should be able to run the code cell independently, load all the libraries you need as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomForest model is being implemented here as new classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Holdout and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Random Forest model on Holdout-CV data:\n",
      "98.18182\n",
      "Accuracy of the Random Forest model on test data:\n",
      "97.66454\n"
     ]
    }
   ],
   "source": [
    "##Holdout Cross-validation method\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=10, stratify=Y)\n",
    "x_train_cv, x_test_cv, y_train_cv, y_test_cv = train_test_split(x_train, y_train, random_state=10, \n",
    "                                                                test_size=0.3, stratify=y_train)\n",
    "\n",
    "randforest_holdout_cv = RandomForestClassifier()\n",
    "\n",
    "randforest_holdout_cv.fit(x_train_cv, y_train_cv)\n",
    "\n",
    "randforest_holdout_cv_pred = randforest_holdout_cv.predict(x_test_cv)\n",
    "\n",
    "print('Accuracy of the Random Forest model on Holdout-CV data:')\n",
    "print(round(metrics.accuracy_score(y_test_cv, randforest_holdout_cv_pred) * 100, 5))\n",
    "\n",
    "\n",
    "randforest_holdout_cv_test_pred = randforest_holdout_cv.predict(x_test)\n",
    "\n",
    "print('Accuracy of the Random Forest model on test data:')\n",
    "print(round(metrics.accuracy_score(y_test, randforest_holdout_cv_test_pred) * 100, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Feature reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Random Forest model using feature reduction - PCA:\n",
      "76.85775\n"
     ]
    }
   ],
   "source": [
    "#RandomForest Using Feature Reduction - PCA\n",
    "\n",
    "randforest_pca = RandomForestClassifier()\n",
    "\n",
    "randforest_pca.fit(x_train_pca, y_train)\n",
    "\n",
    "randforest_pca_pred = randforest_pca.predict(x_test_pca)\n",
    "\n",
    "print('Accuracy of the Random Forest model using feature reduction - PCA:')\n",
    "print(round(metrics.accuracy_score(y_test, randforest_pca_pred) * 100, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Feature Normalisation - Peforming Feature reduction on the above normalised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=10, stratify=Y)\n",
    "\n",
    "\n",
    "#Feature reduction\n",
    "\n",
    "pca = PCA(n_components = 30)\n",
    "\n",
    "np.random.seed(42)\n",
    "x_train_pca_rf = pca.fit_transform(x_train)\n",
    "x_test_pca_rf = pca.fit_transform(x_test)\n",
    "\n",
    "\n",
    "\n",
    "#Feature normalisation on the above feature reduction datas\n",
    "np.random.seed(42)\n",
    "\n",
    "x_train_minmax_rf = x_train_pca_rf\n",
    "x_test_minmax_rf = x_test_pca_rf\n",
    "\n",
    "#Using min-max normalisation\n",
    "min_max_scaler_rf = MinMaxScaler()\n",
    "\n",
    "x_train_minmax_rf = min_max_scaler_rf.fit_transform(x_train_minmax_rf)\n",
    "x_test_minmax_rf = min_max_scaler_rf.fit_transform(x_test_minmax_rf)\n",
    "\n",
    "\n",
    "#Using Z-score Normalisation\n",
    "stnd_rf_scalar = StandardScaler()\n",
    "\n",
    "x_train_stnd_rf = x_train_pca_rf\n",
    "x_test_stnd_rf = x_test_pca_rf\n",
    "\n",
    "x_train_stnd_rf = stnd_rf_scalar.fit_transform(x_train_stnd_rf)\n",
    "x_test_stnd_rf = stnd_rf_scalar.fit_transform(x_test_stnd_rf)\n",
    "\n",
    "\n",
    "#Features reduction is done using PCA and the dataset is then normalised using both the Minmax normalisation and \n",
    "#Z-score (or) Standard normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Random Forest model using PCA and min-max normalisation on features:\n",
      "60.08493\n",
      "Accuracy of the Random Forest model using PCA and Standard normalisation on features:\n",
      "80.67941\n"
     ]
    }
   ],
   "source": [
    "#RandomForest on above Feature reduced and Feature normalised data\n",
    "\n",
    "\n",
    "#Min-max normalisation\n",
    "randforest_minmax_normalised = RandomForestClassifier()\n",
    "randforest_minmax_normalised.fit(x_train_minmax_rf, y_train)\n",
    "randforest_minmax_normalised_pred = randforest_minmax_normalised.predict(x_test_minmax_rf)\n",
    "\n",
    "print('Accuracy of the Random Forest model using PCA and min-max normalisation on features:')\n",
    "print(round(metrics.accuracy_score(y_test, randforest_minmax_normalised_pred) * 100, 5))\n",
    "\n",
    "\n",
    "#Standard normalisation\n",
    "randforest_stand_normalised = RandomForestClassifier()\n",
    "randforest_stand_normalised.fit(x_train_stnd_rf, y_train)\n",
    "randforest_stand_normalised_pred = randforest_stand_normalised.predict(x_test_stnd_rf)\n",
    "\n",
    "print('Accuracy of the Random Forest model using PCA and Standard normalisation on features:')\n",
    "print(round(metrics.accuracy_score(y_test, randforest_stand_normalised_pred) * 100, 5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Hyperparameter tuning on above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for this model are:  {'criterion': 'entropy', 'max_depth': 500, 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 25}\n",
      "\n",
      "Accuracy of the Random Forest model after tuning the hyperparameters on the above pre-processed data:\n",
      "77.49469\n"
     ]
    }
   ],
   "source": [
    "rf_param_grid = {'criterion':['gini','entropy'],\n",
    "          'n_estimators':[25,30,40,50],\n",
    "          'max_depth': [100, 500],\n",
    "          'min_samples_leaf':[1,2,3],\n",
    "          'min_samples_split':[5,7,9,11,13,15], \n",
    "          \n",
    "}\n",
    "\n",
    "rf_grid_search = RandomForestClassifier()\n",
    "\n",
    "rf_hyperparam = GridSearchCV(rf_grid_search, param_grid = rf_param_grid, cv = 10)\n",
    "\n",
    "rf_hyperparam.fit(x_train_stnd_rf,y_train)\n",
    "\n",
    "print('Best parameters for this model are: ', rf_hyperparam.best_params_)\n",
    "rf_hyperparam_pred = rf_hyperparam.predict(x_test_stnd_rf)\n",
    "print('\\nAccuracy of the Random Forest model after tuning the hyperparameters on the above pre-processed data:')\n",
    "print(round(metrics.accuracy_score(y_test, rf_hyperparam_pred) * 100, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model results - Random Forest classifier\n",
    "Best parameters for this model are:  {'criterion': 'entropy', 'max_depth': 100, 'min_samples_leaf': 1, 'min_samples_split': 7, 'n_estimators': 25}\n",
    "\n",
    "Accuracy of the Random Forest model after tuning the hyperparameters on the above pre-processed data:\n",
    "80.04246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPORT DATAFRAME\n",
    "\n",
    "\n",
    "report_data_rf = {'Techniques': ['Hold-out and cross-validation', 'Hold-out and cross-validation Test data',\n",
    "                              'Feature reduction - PCA', 'Feature Normalisation - minmax on PCA ', 'Feature Normalisation - Standard on PCA',\n",
    "                             'Final Model (Hyperparameter on pre-proc. data)'], \n",
    "               \n",
    "               'Random Forest Classifer': [98.18182,97.66454,76.85775,60.08493,80.67941,77.49469]}  \n",
    "\n",
    "report_rf = pd.DataFrame(report_data_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PLEASE RUN THE BELOW CELL FOR THE RESULTS DISPLAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Techniques</th>\n",
       "      <th>Random Forest Classifer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hold-out and cross-validation</td>\n",
       "      <td>98.18182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hold-out and cross-validation Test data</td>\n",
       "      <td>97.66454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feature reduction - PCA</td>\n",
       "      <td>76.85775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feature Normalisation - minmax on PCA</td>\n",
       "      <td>60.08493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feature Normalisation - Standard on PCA</td>\n",
       "      <td>80.67941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Final Model (Hyperparameter on pre-proc. data)</td>\n",
       "      <td>77.49469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Techniques  Random Forest Classifer\n",
       "0                   Hold-out and cross-validation                 98.18182\n",
       "1         Hold-out and cross-validation Test data                 97.66454\n",
       "2                         Feature reduction - PCA                 76.85775\n",
       "3          Feature Normalisation - minmax on PCA                  60.08493\n",
       "4         Feature Normalisation - Standard on PCA                 80.67941\n",
       "5  Final Model (Hyperparameter on pre-proc. data)                 77.49469"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report on Task 3\n",
    "\n",
    "It is clearly evident from the results table the Random forest classifier performed better on the holdout crossvalidation method with an highest accuracy of 98.18%. While the accuracy was reduced further when the above techniques were applied on the data as well on the model, the final model accuracy was increasing with the best parameter search.\n",
    "\n",
    "It is evident that the model will perform better with more rigorous gridsearch of hyperparameters. Due to local resource crunch (CPU, RAM, GPU etc), only fewer parameters were involved in model search. When the best set of parameters is found, the model accuracy will improve and it will predict the test data with higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q01BjiiCJTR4"
   },
   "source": [
    "# <font color=\"blue\">FOR GRADING ONLY</font>\n",
    "\n",
    "Save your best model into your github. And create a single code cell that loads it and evaluate it on the following test dataset: \n",
    "https://github.com/andvise/DataAnalyticsDatasets/blob/main/dm_assignment2/sat_dataset_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWx4lyuQI929"
   },
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "# INSERT YOUR MODEL'S URL\n",
    "mLink = 'URL_OF_YOUR_MODEL_SAVED_IN_YOUR_GITHUB_REPOSITORY?raw=true'\n",
    "mfile = BytesIO(requests.get(mLink).content)\n",
    "model = load(mfile)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-k6H-1ZKTYo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CS3033/CS6405 - Data Mining - Second Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
